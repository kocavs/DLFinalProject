# DLFinalProject
The goal is to finetune the mdoels RoBerta, Bert and DistilBert for sentiment classification tasks. By implementing the tweet_eval dataset and three pretrained models with transformer, we conclude that different language models can have varying levels of ability to interpret emotions.

## Prerequisites:
```
pip install transformers
pip install datasets
```

## Pretrained model and dataset used:
* RoBERTa-base, xxx, xxx
* xxx dataset


## Dataset: yelp_review_full
Epoch:  1<br>
        Train Loss: 0.14475 | Train Acc: 47.50%<br>
        Test. Loss: 0.12069 |  Test Acc: 55.20%<br>
        Time: 265.28 seconds<br>
Epoch:  2<br>
        Train Loss: 0.10432 | Train Acc: 63.06%<br>
        Test. Loss: 0.12071 |  Test Acc: 58.25%<br>
        Time: 264.03 seconds<br>
Epoch:  3<br>
        Train Loss: 0.06848 | Train Acc: 77.38%<br>
        Test. Loss: 0.16502 |  Test Acc: 53.15%<br>
        Time: 263.77 seconds<br>
Epoch:  4<br>
        Train Loss: 0.03418 | Train Acc: 90.19%<br>
        Test. Loss: 0.16822 |  Test Acc: 59.55%<br>
        Time: 263.22 seconds<br>
Epoch:  5<br>
        Train Loss: 0.01243 | Train Acc: 96.97%<br>
        Test. Loss: 0.20827 |  Test Acc: 60.15%<br>
        Time: 263.61 seconds<br>

